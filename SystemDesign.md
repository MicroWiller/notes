# 系统设计





## 并发和并行的区别

|            并发            |             并行             |
| :-----------------------: | :-------------------------: |
|         同一时间段         |           同一时刻           |
|    同一实体上的多个事件     |     不同实体上的多个事件      |
| 一台处理器上“同时”处理多个任务 | 多台处理器上同时处理多个任务 |





## 性能优化



### 集群

* 将多台服务器组成集群，使用`负载均衡`（nginx）将请求转发到集群中，避免单一服务器的负载压力过大导致性能降低
* 为了使得大型网站具有`伸缩性`，集群中的 **应用服务器**通常需要保持`无状态`
    1.  应用服务器不能存储用户的会话信息 session



#### 负载均衡

1. 集群中的`应用服务器（节点）`通常被设计成**无状态**，用户可以请求任何一个节点
2. 负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上



##### 负载均衡算法

|     |         轮询         |    加权轮询     |        最少连接         | 加权最少连接 | 随机 |  源地址哈希  |
| :-: | :------------------: | :-------------: | :--------------------: | :---------: | :-: | :---------: |
| 区别 | 轮流发送到每个服务器上 | 服务器的性能差异 | 每个请求的连接时间不一样 |             |     | 实现会话粘滞 |



##### 转发实现











#### 集群下的 Session 管理

|     |     Sticky Session     |         Session Replication         |     Session Server      |
| :-: | :--------------------: | :---------------------------------: | :---------------------: |
| 实现 | 哈希(IP)取模(服务器数量) | 同步：每个服务器都有所有用户的 Session | 单独的服务器存储 Session |
|     |                        |                                     |                         |







### 缓存

缓存能够提高性能的原因如下：
- 缓存数据通常位于`内存等介质`中，这种介质对于读操作特别快；
- 缓存数据可以位于靠近用户的地理位置上；
- 可以将计算结果进行缓存，从而`避免重复计算`。





#### 缓存特征

1. 命中率
2. 最大空间
3. 淘汰策略

    |     |  FIFO（First In First Out）  | LRU（Least Recently Used） | LFU（Least Frequently Used） |
    | :-: | :-------------------------: | :------------------------: | :--------------------------: |
    |     | 先进先出策略，在实时性的场景下 |    最近`最久`未使用策略     |      `最不经常`使用策略       |





#### 缓存技术

位置：
1. 客服端
2. 应用服务器
3. 数据库服务器
4. java内部的缓存：字符串常量池、及 Byte、Short、Character、Integer、Long、Boolean 这六种包装类缓冲池
5. CPU多级缓存





#### CDN：内容分发网络

利用`更靠近用户的服务器`从而更快更可靠地将 HTML、CSS、JavaScript、音乐、图片、视频等`静态资源`分发给用户





#### 缓存问题

1. 缓存穿透
2. 缓存雪崩
3. 缓存一致性
    - 要保证缓存一致性需要付出很大的代价
4. 缓存“无底洞”现象





#### 数据分布

1. 传统的哈希分布
2. 顺序分布：将数据划分为多个连续的部分，按数据的 ID 或者时间分布到不同节点上
3. 一致性哈希





### 异步

> 某些流程可以`将操作转换为消息`，将消息发送到`消息队列`之后立即返回，之后`这个操作会被异步处理`





#### 消息模型

|     |             点对点              |                  发布/订阅                   |             观察者              |
| :-: | :-----------------------------: | :------------------------------------------: | :-----------------------------: |
| 区别 | 异步？？发送一个消息只能被消费一次 | 异步，多个消费者可以从该频道订阅到这条消息并消费 | 同步，观察者和主题都知道对方的存在 |

> 还有助于 流量削峰 和 应用解耦













## 技术演进

[淘宝技术演进](https://www.cnblogs.com/52czm/p/11097156.html
)	



[大型应用架构演进历程](https://mp.weixin.qq.com/s/jtBTv1rsFHDDlyqbaWI-bA) 











## 框架设计的套路



 [接口和抽象类：servlet|servlet容器|web容器](https://www.cnblogs.com/yescode/p/14099868.html)	















## 数据库扩容

[分库分表以后，如何实现扩容？？](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=69#/detail/pc?id=1924)：

1. 对主键进行哈希取模

   - 数据拆分比较均匀，但不利于后续的扩容

2. 基于数据范围进行路由

   - 扩容可以直接增加新的存储，将新生成的数据区间映射到新添加的存储节点中，不需要进行节点之间的调整，也不需要迁移历史数据
   - 缺点就是数据访问不均匀，出现数据库访问瓶颈

3. 结合哈希和数据范围的分库分表规则

   - 先对订单 ID 进行哈希取模，然后对取模后的数据再次进行范围分区

     ![](https://raw.githubusercontent.com/MicroWiller/photobed/master/DatabaeMigration.png)

   - 哈希取模结合数据区间的方式，可以比较好地平衡两种路由方案的优缺点

   - 避免了单纯基于数据范围可能出现的热点存储

   - 后期扩展时，可以直接增加对应的`扩展表`，避免了复杂的数据迁移工作







## 系统边界



- 一个中高级研发工程师对系统的驾驭边界至少是**模块**或者**子系统**层面；

- 一个架构师对系统的驾驭边界至少是**全系统层面**；

- 一个高级架构师对系统的驾驭边界至少是**某一领域**层面。



管理者是通过制度来管理员工之间的协作，架构师是通过技术来管理系统之间的协作





### 架构设计方案



#### 复杂来源

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/ComplexOfSystem.png" style="zoom:67%;" />







#### 解决方案

1. 设计两到三套备选方案，考虑通过不同的技术方式来解决问题。
2. 方案设计不用过于详细，而是要确定技术的可行性和优缺点。



#### 评估标准



​		通用的设计原则：设计松耦合、系统可监控、系统无单点、系统可降级、可水平扩展



​		全局性关注点，比如性能、可用性、IT 成本、投入资源、实现复杂度、安全性、后续扩展性等



​	

扩充：系统可降级常用手段主要有三种

1. 限流，即抛弃超出预估流量外的用户。

2. 降级，即抛弃部分不重要的功能，让系统提供有损服务，如商品详情页不展示宝贝收藏的数量，以确保核心功能不受影响。

3. 熔断，即抛弃对故障系统的调用。一般情况下熔断会伴随着降级处理，比如展示兜底数据。



方案没有优劣之分，而是要看哪个更适合当下的问题，只要架构**满足一定时期内的业务发展**就可以



#### 技术实现



需要进一步说明技术上的落地实现方式和深层原理，如果你最终选择基于 Redis 来实现消息队列，那么可以有几种实现方式？各自的优缺点有哪些？对于这些问题，要做到心里有数。



比如: 

- 基于 Redis List 的 LPUSH 和 RPOP 的实现方式
- 基于 Redis 的订阅或发布模式
- 基于 Redis 的有序集合（Sorted Set）的实现方式







## 分布式技术原理与设计

> 互联网分布式的设计方案是**数据一致性**和**系统可用性**的权衡

- 即使无法做到强一致性（简单来讲强一致性就是在任何时刻所有的用户查询到的数据都是最新的）

- 也可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性



参考：[分布式技术章节-拉钩](https://kaiwu.lagou.com/course/courseInfo.htm?courseId=592#/detail/pc?id=6052) 



[分布式和集群的区别及分布式概述](https://www.cnblogs.com/siyuanwai/p/14143554.html) 



### CAP



> CAP 理论指的是：
>
> C（Consistency）是数据一致性、A（Availability）是服务可用性、P（Partition tolerance）是分区容错性



 [CAP(Consistency、Availability、Partition tolerance)](https://mp.weixin.qq.com/s/X_UxMRCfSGmX9xfj_l6FlQ) : 分布式系统，**当发生分区错误时**，如何选择一致性和可用性。



- C、A、P 只能同时满足两个目标
- 而由于在分布式系统中，P 是必须要保留的(网络问题导致的网络分区是常态)，所以要在 C 和 A 间进行取舍
- 假如要保证服务的可用性，就选择 AP 模型
- 而要保证一致性的话，就选择 CP 模型





###  PACELC新模型

 [PACELC](http://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf) 

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/PACELC.png" style="zoom:67%;" />



- 如果有网络分区产生，系统就必须在 A 和 C 之间取得平衡
- 否则（Else，即 PACELC 中的 E）当系统运行在无网络分区情况下，系统需要在 L（**延迟**）和 C 之间取得平衡







### BASE 理论

> BASE 理论，是 CAP 理论的延伸

- Basically Available（基本可用）、Soft State（软状态）和 Eventually Consistent（最终一致性）
- 作用是保证系统的**可用性**，然后通过最终一致性来**代替**强一致性
- 它是目前分布式系统设计中最具指导意义的经验总结
- 软状态和最终一致性指的是*允许系统中的数据存在中间状态*，这是为了系统可用性而牺牲一段时间窗内的数据一致性，从而保证最终的数据一致性的做法
- 最经典的例子是在用户下单的时候不需要真正地扣减库存，而是仅在前台计个数，然后通过异步任务在后台批量处理



BASE 中的基本可用指的是*保障核心功能* 的**基本可用**，其实是做了“可用性”方面的妥协，比如：

- 电商网站在双十一大促等访问压力较大的时候，关闭商品排行榜等次要功能的展示，从而保证商品交易主流程的可用性，这也是我们常说的服务降级；

- 为了错开双十一高峰期，电商网站会将预售商品的支付时间延后十到二十分钟，这就是流量削峰；

- 抢购商品的时候，往往会在队列中等待处理，这也是常用的延迟队列。





### 分布式系统理解



分布式系统看起来就像一个计算机

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/DisruptSystem.png" style="zoom:67%;" />



- 计算与存储由**一系列网络节点**组成，每个节点之间的通信就是输入与输出，各节点之间的调度管理就是控制器



分布式系统就像一个网络计算机，它的知识体系包括四个角度：

1. 存储器，即分布式存储系统，如 NoSQL 数据库存储
2. 运算器，即分布式计算，如分布式并行计算
3. 输入输出，即分布式系统通信，如同步 RPC 调用和异步消息队列
4. 控制器，即调度管理，如流量调度、任务调度与资源调度





### 分布式存储

- 如何设计一个支持海量商品存储的高扩展性架构？

- 在做分库分表时，基于 Hash 取模和一致性 Hash 的数据分片是如何实现的？

- 在电商大促时期，如何对热点商品数据做存储策略 ？

- 强一致性和最终一致性的数据共识算法是如何实现的 ？



一、数据的水平扩展: 

- 在互联网业务场景下，为了解决单台存储设备的局限性，会把数据分布到多台存储节点上，以此实现数据的水平扩展
- 既然要把数据分布到多个节点，就会存在数据分片的问题
- 数据分片即按照一定的规则将数据路由到相应的存储节点中，从而降低单存储节点带来的读写压力
- 常见的实现方案有 Hash（哈希分片）与 Range（范围分片）



二、副本

- 明确了如何分片后，就需要对数据进行复制，数据复制会产生副本
- **副本**是分布式系统解决高可用的*唯一手段*，也就是主从模式，master-slave



三、数据一致性

- 考虑一致性强弱（即强一致性和最终一致性的问题）
- 一致性协议：如两阶段提交协议（Two-Phrase Commit，2PC）、Paxos 协议选举、Raft 协议、Gossip 协议

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/DistributeStore.png" style="zoom:67%;" />





#### 分片规则



分片有Hash / 一致性hash / 范围分片(可以加入业务属性)等：

- 分片是应对**写热点**的一种策略
- 但具体哪个分片规则更合适，要站在业务场景来分析



**Hash分片：**

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/HashSharding.png" style="zoom:67%;" />



- Hash 分片的优点在于可以保证数据非常均匀地分布到多个分片上，并且实现起来简单
- 但扩展性很差，因为分片的计算方式就是直接用节点取模，节点数量变动，就需要重新计算 Hash，就会导致大规模数据迁移的工作



**一致性hash分片：**

> 概念：
>
> ​	指将存储节点和数据都映射到一个首尾相连的哈希环上。
>
> ​	存储节点一般可以根据 IP 地址进行 Hash 计算，数据的存储位置是从数据映射在环上的位置开始，依照顺时针方向所找到的第一个存储节点。

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/ConsistencyHashSharding.png" style="zoom:67%;" />



- 当我们新增一台服务器，即节点 E 时，受影响的数据仅仅是新服务器到所处环空间中前一台服务器（即沿着逆时针方向的第一台服务器）之间的数据。

- 一致性 Hash 分片的优点是数据可以较为均匀地分配到各节点，其并发写入性能表现也不错。
- 一致性 Hash 提升了稳定性，使节点的加入和退出不会造成大规模的数据迁移
- 但本质上 Hash 分片是一种**静态的**分片方式，必须要提前设定分片的最大规模，而且*无法避免单一热点问题*， 某一数据被海量并发请求后，不论如何进行 Hash，*数据也只能存在一个节点上*，这势必会带来热点请求问题。
- 比如电商中的商品，如果某些商品卖得非常火爆，通过 Hash 分片的方式很难针对热点商品做单独的架构设计





如何解决单一热点问题？

答案是做 Range（范围）分片。 



**Range（范围）分片：**

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/RangeSharding.png" style="zoom:67%;" />



- 与 Hash 分片不同的是，Range 分片能结合业务逻辑规则
- 例如，用 “Category（商品类目）” 作为关键字进行分片时，不是以统一的商品一级类目为标准，而是可以按照一、二、三级类目进行灵活分片
- 一个简单的实现方式是：预先设定主键的生成规则，根据规则进行数据的分片路由，但这种方式会*侵入*商品各条线主数据的业务规则
- 更好的方式是基于*分片元数据*（不过架构设计没有好坏，只有适合与否）





<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/MetaDataSharding.png" style="zoom:67%;" />



- 元数据也是数据，特殊之处在于它类似一个*路由表*，每一次请求都要访问它





ETCD 是如何解决数据共识问题的？为什么要选择这种数据复制方式呢？

- ETCD 的共识算法是基于 Raft 协议实现的强一致性算法，同类的强一致性算法还有 Paxos



Paxos 算法解决了什么问题？

Basic Paxos 算法的工作流程是什么？

Paxos 算法和 Raft 算法的区别又是什么？



背景：在分布式系统中，造成系统不可用的场景很多，比如服务器硬件损坏、网络数据丢包等问题，解决这些问题的根本思路是多副本，副本是分布式系统解决高可用的唯一手段，也就是主从模式



那么如何在*保证一致性*的前提下，提高系统的可用性？？

- Paxos 就被用来解决这样的问题



Paxos 又分为 Basic Paxos 和 Multi Paxos，然而因为它们的实现复杂，工业界很少直接采用 Paxos 算法，所以 ETCD 选择了 Raft 算法



> Raft 是 Multi Paxos 的一种实现，是通过一切以领导者为准的方式，实现一系列值的共识，然而不是所有节点都能当选 Leader 领导者，Raft 算法对于 Leader 领导者的选举是有限制的，只有**最全的日志节点**才可以当选。

TiDB 就是基于 Raft 算法的优化







分片元数据服务毕竟是一个中心化的设计思路，而且基于强一致性的共识机制还是可能存在性能的问题，有没有更好的架构思路呢？？

- 既然要*解决可用性的问题*，根据 Base 理论，需要实现最终一致性
- 那么 Raft 算法就不适用了，因为 <u>Raft 需要保证大多数节点正常运行后才能运行</u>
- 这个时候，可以选择基于 **Gossip** 协议的实现方式。



Gossip 的协议原理：

- 有一种传播机制叫谣言传播
- 指的是当一个节点有了新数据后，这个节点就变成了活跃状态，并周期性地向其他节点发送新数据，直到所有的节点都存储了该条数据
- 这种方式达成的数据一致性是 “最终一致性”，即执行数据更新操作后，经过一定的时间，集群内各个节点所存储的数据最终会达成一致，*很适合动态变化的分布式系统*

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/GossipProtocal.png" style="zoom:67%;" />





> 一致性共识算法总结：
>
> 1. 共识算法的选择和数据副本数量的多少息息相关
> 2. 如果副本少、参与共识的节点少，推荐采用广播方式，如 Paxos、Raft 等强一致性？？协议
> 3. 如果副本多、参与共识的节点多，那就更适合采用 Gossip 这种最终一致性协议

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/ConsistencySum.png" style="zoom:67%;" />







 **热点数据**分为*读* 和*写* 两种情况：

- 读热点很好解决，加一个缓存就能抗住
- 写热点就是做分库分表，当分库分表的规则就要立足于具体的业务场景
- 读操作：缓存和限流是任何时刻都需要做的，不分场景
- 写操作：对于单一热点品类的数据存储写入问题，要做到有能力承载







**共识算法**是一致性的方案，**强一致性**是共识的状态





一致性 写大多数副本，性能会变慢；一致性和时延，总有一个在路上







### 分布式事务



分布式事务是指：

- 一次大的操作由多个小操作组成，这些小的操作分布在不同的服务器上
- 分布式事务需要保证这些小操作要么全部成功，要么全部失败



很典型的分布式事务问题，解决方案很多，有两阶段提交协议（Two-Phase Commit，2PC）、3PC 、TCC 和基于消息队列的实现方式。





#### 2PC



2PC 是分布式事务教父级协议，它是数据库领域解决分布式事务最典型的协议。



它的处理过程分为准备和提交两个阶段，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成：

- 协调者就是事务管理器
- 参与者就是具体操作执行的资源管理器





假设订单数据，商品数据和促销数据分别保存在数据库 D1，数据库 D2 和数据库 D3 上：



一、**准备阶段**

​		事务管理器首先通知所有资源管理器开启事务，询问是否做好提交事务的准备。

​		如资源管理器此时会将 undo 日志和 redo 日志计入事务日志中，并做出应答，当协调者接收到反馈 Yes 后，则准备阶段结束。

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/TwopcPerpare.png" style="zoom:67%;" />



二、**提交阶段**

​		当收到所有数据库实例的 Yes 后，事务管理器会发出提交指令。

​		每个数据库接受指令进行本地操作，正式提交更新数据，然后向协调者返回 Ack 消息，事务结束。

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/TwopcCommit.png" style="zoom:67%;" />





三、**中断阶段**

​		如果任何一个参与者向协调者反馈了 No 响应

​		例如用户 B 在数据库 D3 上面的余额在执行其他扣款操作，导致数据库 D3 的数据无法锁定，则只能向事务管理器返回失败。

​		此时，协调者向所有参与者发出 Rollback 请求，参与者接收 Rollback 请求后，会利用其在准备阶段中记录的 undo 日志来进行回滚操作，并且在完成事务回滚之后向协调者发送 Ack 消息，完成事务回滚操作。

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/TwopcInterupt.png" style="zoom:67%;" />





2PC 可以借助数据库的本地事务操作，实现起来较为简单，不用侵入业务逻辑，但是它也**存在着很多问题**：

1. 2PC 在准备阶段会要求每个资源管理器进行资源锁定，如 MySQL 的行锁。

2. 否则如果在提交阶段提交之前数据发生改变，就会出现数据不一致的情况。

   原因：如果商品库存数据为 1，也就是数据库 D1 为 1，在准备阶段询问是否可以扣减库存，商品数据返回可以，此时如果不锁定数据，在提交阶段之前另外一个请求去扣减了数据库 D1 的数据，这时候，在提交阶段再去扣减库存时，数据库 D1 的数据就会超售变成了负 1。

3. 但正因为要**加锁**，会导致两阶段提交存在一系列问题，最严重的就是*死锁问题* 

4. 一旦发生故障，数据库就会阻塞，*尤其在提交阶段*，如果发生故障，数据都还处于资源锁定状态，将无法完成后续的事务提交操作。

5. 其次是**性能问题**，数据库（如 MySQL ）在执行过程中会对操作的数据行执行数据行锁，如果此时其他的事务刚好也要操作被锁定的数据行，那它们就只能阻塞等待，*使分布式事务出现高延迟和性能低下*。

6. 再有就是**数据不一致性**，在提交阶段，当事务管理器向参与者发送提交事务请求之后，如果此时出现了*网络异常*，只有部分数据库接收到请求，那么会导致未接收到请求的数据库无法提交事务，整个系统出现数据不一致性。







#### 基于 MQ 的可靠消息投递方案 



在应对高并发场景下的分布式事务问题时，一种方案通过*放弃强一致性*，*而选择最终一致性*，来提高系统的可用性。

> 基于 MQ 的可靠消息队列投递方案是目前互联网最为常用的方式

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/DistributeTranscationByMQ.png" style="zoom:67%;" />



- 还是拿下单场景举例
- 当订单系统调用优惠券系统时，将扣减优惠券的事件放入消息队列中
- 最终给优惠券系统来执行，然后只要保证事件消息能够在优惠券系统内被执行就可以了
- 因为消息已经持久化在消息中间件中，即使消息中间件发生宕机，我们将它重启后也不会出现消息丢失的问题



基于 MQ 的可靠消息投递的方案**不仅可以** *解决由于业务流程的同步执行而造成的阻塞问题*，**还可以** *实现业务解耦合 流量削峰*。



1. MQ <u>自动应答机制</u>导致的消息丢失

   1. 背景：订阅消息事件的优惠券服务在接收订单服务投递的消息后，消息中间件（如 RabbitMQ）默认是开启消息自动应答机制，当优惠券系统消费了消息，消息中间件就会删除这个持久化的消息。
   2. 导致：但在优惠券系统执行的过程中，很可能因为执行异常导致流程中断，那这时候消息中间件中就没有这个数据了，进而会导致消息丢失。
   3. 解决：要采*取编程的方式手动发送应答*，也就是当优惠券系统执行业务成功之后，消息中间件才能删除这条持久化消息。

2. 高并发场景下的消息积压导致消息丢失

   1. 背景：（分布式部署环境基于网络进行通信，而在网络通信的过程中，上下游可能因为各种原因而导致消息丢失。） 比如优惠券系统由于*流量过大*而<u>触发限流</u>，不能保证事件消息能够被及时地消费，这个消息就会被消息队列不断地重试，最后可能由于超过了最大重试次数而被丢弃到死信队列中。
   2. 分析：这个问题源于订单系统作为事件的生产者进行消息投递后，*无法感知* 它下游（即优惠券系统）的所有操作，那么优惠券系统作为事件的消费者，是消费成功还是消费失败，订单系统并不知道。
   3. 分析解决：顺着这个思路，如果让订单知道消费执行结果的响应，即使出现了消息丢失的情况，订单系统也还是可以通过定时任务扫描的方式，将未完成的消息重新投递来进行消息补偿。
   4. 解决方案：队列双向确认

   <img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/QueueDoubleAck.png" style="zoom:67%;" />

   - 先让订单系统把要发送的消息持久化到本地数据库里，然后将这条消息记录的状态设置为代发送
   - 紧接着订单系统再投递消息到消息队列，优惠券系统消费成功后，也会向消息队列发送一个通知消息
   - 当订单系统接收到这条通知消息后，再把本地持久化的这条消息的状态设置为完成
   - 这样做后，即使最终 MQ 出现了消息丢失，也可以通过*定时任务* 从订单系统的本地数据库中扫描出一段时间内未完成的消息，进行重新投递，最终保证订单系统和优惠券系统的最终事务一致性。





> 在实际工作中，并不是所有的业务对事务一致性的要求都那么高。
>
> 因为更高的要求意味着更多的成本，这也是很多架构复杂度来源之一，所以要尽可能地站在业务实际场景的立足点来分析分布式事务问题。





### 分布式锁



概念：在分布式环境下，多个系统在同时操作**共享资源**（如写数据）时，发起操作的系统通常会通过一种方式去协调其他系统，然后获取访问权限，*得到访问权限后* 才可以写入数据，其他系统必须等待权限释放。

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/DistributedLock.png" style="zoom:67%;" />





分布式锁经常出现哪些问题？？以及如何解决？？

- 可用问题：无论何时都要保证锁服务的可用性（这是系统正常执行锁操作的基础）
- 死锁问题：客户端一定可以获得锁，即使锁住某个资源的客户端在释放锁之前崩溃或者网络不可达（这是避免死锁的设计原则）
- 脑裂问题：集群同步时产生的数据不一致，导致新的进程有可能拿到锁，但之前的进程以为自己还有锁，那么就出现<u>两个进程拿到了同一个锁的问题</u> 





为了解决可用性、死锁、脑裂等问题，一般还会再考虑一下锁的四种设计原则：

- 互斥性：即在分布式系统环境下，对于某一共享资源，需要保证在同一时间只能一个线程或进程对该资源进行操作。

- 高可用：也就是可靠性，锁服务不能有单点风险，要保证分布式锁系统是集群的，并且某一台机器锁不能提供服务了，其他机器仍然可以提供锁服务。

- 锁释放：具备锁失效机制，防止死锁。即使出现进程在持有锁的期间崩溃或者解锁失败的情况，也能被动解锁，保证后续其他进程可以获得锁。

- 可重入：一个节点获取了锁之后，还可以再次获取整个锁资源。









#### 基于关系型数据库实现分布式锁



##### 悲观锁



1. 先查询数据库是否存在记录，为了防止幻读取，通过数据库行锁 select for update 锁住这行数据

```mysql
select id from order where order_id = xxx for update
```

2. 然后将查询和插入的 SQL 在同一个事务中提交





要注意，基于 MySQL 行锁的方式会出现交叉死锁：

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/InterSectLockByMysql.png" style="zoom:67%;" />



- 事务 1 和事务 2 分别取得了记录 1 和记录 2 的排它锁
- 然后事务 1 又要取得记录 2 的排它锁
- 事务 2 也要获取记录 1 的排它锁
- 那这两个事务就会因为相互锁等待，产生死锁



解决方法：可以通过“超时控制”解决交叉死锁的问题

- 但在高并发情况下，出现的大部分请求都会排队等待
- 所以“基于关系型数据库实现分布式锁”的方式**在性能上**存在缺陷







##### 乐观锁



基于版本号的方式：

- 首先在数据库增加一个 *int 型字段 ver*
- 然后在 SELECT 同时获取 ver 值
- 最后在 UPDATE 的时候检查 ver 值是否为与第 2 步或得到的版本值相同

```mysql
## SELECT 同时获取 ver 值
select amount, old_ver from order where order_id = xxx
## UPDATE 的时候检查 ver 值是否与第 2 步获取到的值相同
update order set ver = old_ver + 1, amount = yyy where order_id = xxx and ver = old_ver
```

- 如果更新结果的记录数为1，就表示成功
- 如果更新结果的记录数为 0，就表示已经被其他应用更新过了，*需要做异常处理*







#### 基于分布式缓存实现分布式锁



> 将数据仅存放在系统的内存中，不写入磁盘，从而减少 I/O 读写



```shell
SET lock_key unique_value NX PX 10000
```



- lock_key 就是 key 键
- unique_value 是客户端生成的唯一的标识
- NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作
- PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。



解锁的过程就是将 lock_key 键删除，*但不能乱删*，要保证执行操作的客户端就是加锁的客户端

**unique_value** 的作用就体现出来，实现方式可以通过 *lua* 脚本判断 unique_value 是否为加锁客户端

```lua
// 释放锁时，先比较 unique_value 是否相等，避免锁的误释放
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```









##### 基于 Redis 实现分布式锁的优缺点



基于缓存实现的分布式锁主要的优点主要有三点：

1. **性能高效**（这是选择缓存实现分布式锁最核心的出发点）。

2. **实现方便**。
   - 很多研发工程师选择使用 Redis 来实现分布式锁，很大成分上是因为 Redis 提供了 setnx 方法，实现分布式锁很方便。
   - 但是需要注意的是，在 Redis2.6.12 的之前的版本中，由于加锁命令和设置锁过期时间命令是两个操作（不是原子性的）
   - 当出现某个线程操作完成 setnx 之后，还没有来得及设置过期时间，线程就挂掉了，就会导致当前线程设置 key 一直存在，后续的线程无法获取锁，最终造成死锁的问题
   - 所以要选型 Redis 2.6.12 后的版本或通过 Lua 脚本执行加锁和设置超时时间
   - Redis 允许将 Lua 脚本传到 Redis 服务器中执行, 脚本中可以调用多条 Redis 命令，并且 Redis 保证脚本的原子性

3. 避免单点故障（因为 Redis 是跨集群部署的，自然就避免了单点故障）。





基于 Redis 实现分布式锁的缺点：（导致分布式锁的不可靠性）

- 不合理设置超时时间
- Redis 集群的数据同步机制





##### 如何合理设置超时时间

- 通过超时时间来控制锁的失效时间，*不太靠谱*
- 比如在有些场景中，一个线程 A 获取到了锁之后，由于业务代码执行时间可能比较长，导致超过了锁的超时时间，自动失效
- 后续线程 B 又意外的持有了锁，当线程 A 再次恢复后，通过 del 命令释放锁，就错误的将线程 B 中同样 key 的锁误删除了



<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/MistakeOperateByTimeout.png" style="zoom:80%;" />

- 如果锁的超时时间设置过长，*会影响性能*
- 如果设置的超时时间过短，*有可能业务阻塞没有处理完成*
- 能否合理设置超时时间，是基于缓存实现分布式锁**很难解决**的一个问题



解决方案：

- 基于**续约**的方式设置超时时间：
- 先给锁设置一个超时时间，然后启动一个*守护线程* 
- 让守护线程在一段时间后，重新设置这个锁的超时时间
- 实现方式就是：写一个守护线程，然后去判断锁的情况，当锁快失效的时候，再次进行续约加锁，当主线程执行完成后，销毁续约锁即可。



补充：不过这种方式实现起来相对复杂，建议结合业务场景进行回答，所以针对超时时间的设置，*要站在实际的业务场景中进行衡量*。







##### Redis 如何解决集群情况下分布式锁的可靠性？



问题：

- 由于 Redis 集群数据同步到各个节点时是*异步的* 
- 如果在 Redis 主节点获取到锁后，在*没有同步* 到其他节点时，Redis 主节点宕机了
- 此时*新的 `Redis` 主节点* 依然可以获取锁，所以多个应用服务就可以同时获取到锁



解决：`Redis` 官方已经设计了一个分布式锁算法 `Redlock` 解决了这个问题



为了避免 `Redis` 实例故障导致锁无法工作的问题，`Redis` 的开发者 `Antirez` 设计了分布式锁算法 `Redlock`：

- `Redlock` 算法的基本思路，是让客户端和*多个* **独立的** `Redis` 实例依次请求申请加锁
- 如果客户端能够和**半数以上**的实例成功地完成加锁操作，并且**总耗时**没有超过锁的有效时间
- 那么我们就认为，客户端成功地获得分布式锁，否则加锁失败









##### 基于 Zookeeper 实现分布式锁的方式





- `zookeeper`分布式锁是通过**临时顺序节点**来实现的
- 由**`zab`协议**保证锁的互斥性和高可用
- 客户端异常退出后临时顺序节点会失删除，相当于释放锁，并会通知下一个临时节点对应的客户端可以加锁



基于`zookeeper`的分布式锁需要通过共识算法保证一致性，所以*锁的效率不高*





## `RPC`



- `RPC` 的一次调用过程是怎样的？

- `RPC` 的服务发现是如何实现的？

- `RPC` 的负载均衡有哪些？





<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/ExamWithRPC.png" style="zoom:67%;" />



- 各个微服务系统之间通过远程 `RPC` 调用的方式通信



==对于整条 `RPC` 调用链路（从 `App` 到网关再到各个服务系统），怎么设置 `RPC` 的超时时间，要考虑哪些问题？？== 



- 系统整体的平均响应时长，会受到所有依赖服务接口的耗时和重传次数影响。



- **结合 `TP99` 请求耗时**：首先如果要回答“*超时时间设置和重传次数问题*”，需要根据每一个微服务 `TP99` 的请求耗时，以及业务场景进行综合衡量。

- **`RPC` 调用方式**：站在业务场景下，讲清楚网关调用各下游服务的串并行方式，服务之间是否存在上下服务依赖。

- **分析核心服务**：分析出哪些是核心服务，哪些是非核心服务，核心服务是否有备用方案，非核心服务是否有降级策略。



> 总的来说，任何一个微服务出现性能问题，都会影响网关系统的平均响应时长，最终对 `App` 产生影响。









### `RPC`原理

<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/RpcWithQps.png" style="zoom:67%;" />



- 商品详情页的 `QPS` 已达到了 2 万次/s
- 在做了服务化拆分之后，此时完成一次请求需要调用 3 次 `RPC` 服务，计算下来，`RPC` 服务需要承载大概 6 万次/s 的请求。

*那么你怎么设计 `RPC` 框架才能承载 6 万次/s 请求量呢？*  



从两个角度分析：

- 优化 `RPC` 的网络通信性能： 高并发下选择高性能的网络编程 I/O 模型。

- 选型合适的 `RPC` 序列化方式： 选择合适的序列化方式，进而提升封包和解包的性能。









### 一次完整的 `RPC` 流程





<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/RPCWithSocket.png" style="zoom:67%;" />



协议会分成数据头和消息体：

- 数据头一般用于身份识别，包括协议标识、数据大小、请求类型、序列化类型等信息；

- 消息体主要是请求的业务参数信息和扩展属性等。



一次完整的 `RPC` 调用会经过这样几个步骤：

- 调用方 持续把请求参数对象序列化成二进制数据，经过 TCP 传输到服务提供方；

- 服务提供方从 *TCP 通道* 里面接收到二进制数据；

- 根据 `RPC` 协议，**服务提供方** 将二进制数据分割出不同的请求数据，经过反序列化将二进制数据逆向还原出请求对象，找到对应的实现类，完成真正的方法调用；

- 然后服务提供方再把执行结果序列化后，回写到对应的 TCP 通道里面；

- 调用方获取到应答的数据包后，再反序列化成应答对象。





### 如何选型序列化方式



常见序列化方式：

- `JSON`：*Key-Value* 结构的文本序列化框架，易用且应用最广泛，*基于 HTTP 协议的 `RPC` 框架都会选择 `JSON`* 序列化方式，但它的空间开销很大，在通信时需要更多的内存。

- Hessian：一种*紧凑的*二进制序列化框架，在性能和体积上表现比较好。

- `Protobuf`：*Google* 公司的序列化标准，序列化后体积相比 `JSON`、Hessian 还要小，兼容性也做得不错。







==考虑时间与空间开销，切勿忽略兼容性。== 



- 在大量并发请求下，如果序列化的速度慢，势必会增加**请求和响应的时间**（时间开销）。

- 另外，如果序列化后的传输数据体积较大，也会使**网络吞吐量**下降（空间开销）。
- 在 `RPC` 迭代中，常常会因为序列化协议的**兼容性**问题使 `RPC` 框架不稳定
  - 比如某个类型为集合类的入参服务调用者不能解析
  - 某个类的一个属性不能正常调用

- 当然还有安全性、易用性等指标，不过并不是 `RPC` 的关键指标



> 总的来说，在面试时，要综合考虑上述因素，总结出常用序列化协议的选型标准
>
> - 比如首选 Hessian 与 `Protobuf`，因为它们在时间开销、空间开销、兼容性等关键指标上表现良好





### 如何提升网络通信性能



如何提升 `RPC` 的网络通信性能，这句话翻译一下就是：*一个 `RPC` 框架如何选择高性能的网络编程 I/O 模型？*





网络编程中的五个 I/O 模型：

- 同步阻塞 I/O（`BIO`）

- 同步非阻塞 I/O

- I/O 多路复用（`NIO`）

- 信号驱动

- 以及异步 I/O（`AIO`）





#### `BIO` 网络模型



<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/InternetModelByBIO.png" style="zoom:50%;" />



```java
public class BIOSever {
    ServerSocket ss = new ServerSocket();
    // 绑定端口 9090
    ss.bind(new InetSocketAddress("localhost", 9090));
    System.out.println("server started listening " + PORT);
    try {
        Socket s = null;
        while (true) {
            // 阻塞等待客户端发送连接请求
            s = ss.accept();
            new Thread(new ServerTaskThread(s)).start();
        }
    } catch (Exception e) {
        // 省略代码...
    } finally {
        if (ss != null) {
            ss.close();
            ss = null;
    }
}
public class ServerTaskThread implements Runnable {
    // 省略代码...
    while (true) {
        // 阻塞等待客户端发请求过来
        String readLine = in.readLine();
        if (readLine == null) {
            break;
        }
        // 省略代码...
    }
    // 省略代码...
}
```



- 每当客户端发送一个连接请求给服务端，服务端都会启动一个**新的线程**去处理客户端连接的读写操作
- 客户端 Socket 和服务端工作线程的数量是 **1 : 1**，这会导致服务器的*资源不够用*，无法实现高并发下的网络开发





**`BIO` 模型有两处阻塞的地方： ** 

- 服务端*阻塞等待* 客户端发起连接。
  - 在第 11 行代码中，通过 `serverSocket.accept()` 方法服务端等待用户发连接请求过来。



- 连接成功后，工作线程*阻塞读取* 客户端 Socket 发送数据。
  - 在第 27 行代码中，通过 `in.readLine()` 服务端从网络中读客户端发送过来的数据，这个地方也会阻塞。
  - 如果客户端已经和服务端建立了一个连接，但客户端迟迟不发送数据，那么服务端的 `readLine()` 操作会一直阻塞，造成资源浪费。



> 总结下来就两点：
>
> - Socket 连接数量受限，不适用于高并发场景；
>
> - 有两处阻塞，分别是等待用户发起连接，和等待用户发送数据。





#### `NIO` 网络模型



<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/InternetModelByNIO.png" style="zoom:67%;" />



- 用一个线程处理多个连接，使得每一个工作线程都可以*处理多个客户端的 Socket 请求*
- 这样工作线程的利用率就能得到提升，所需的工作线程数量也随之减少





既然服务端的工作线程可以服务于多个客户端的连接请求，*那么具体由哪个工作线程服务于哪个客户端请求呢？* 



- 这时就需要一个**调度者去监控**所有的客户端连接
- 比如当图中的客户端 A 的输入已经准备好后
- 就由这个调度者去通知服务端的工作线程，告诉它们由工作线程 1 去服务于客户端 A 的请求
- 这种思路就是 `NIO` 编程模型的基本原理，调度者就是 **Selector 选择器**



> `NIO` 比 `BIO` 提高了服务端工作线程的利用率，并增加了一个调度者，来实现 Socket 连接与 Socket 数据读写之间的分离。(？？还是需要阻塞等待用户连接？？)



拓展：而 Reactor 模式是基于 I/O 多路复用的	









## `MQ`



> 在使用 `MQ` 的时候，怎么确保消息 100% 不丢失？



如何知道有消息丢失？

哪些环节可能丢消息？

如何确保消息不丢失？







### 引入`MQ`最直接的目的是

- 系统解耦：用 `MQ` 消息队列，可以**隔离** *系统上下游环境变化带来的不稳定因素*  
- 流量控制：遇到秒杀等**流量突增**的场景，通过 `MQ` 还可以实现流量的“*削峰填谷*”的作用，可以根据下游的处理能力*自动调节* 流量
- 追其根源还是为了解决互联网系统的**高可用** 和 **高性能** 问题





### 引入`MQ`也会带来其他问题

- 引入 `MQ` 消息中间件实现系统解耦，会影响系统之间数据传输的一致性

  - 在分布式系统中，如果两个节点之间存在数据同步，就会带来数据一致性的问题
  - 消息生产端和消息消费端的**消息数据一致性**问题（也就是如何确保消息不丢失）

  

- 而引入 `MQ` 消息中间件解决流量控制， 会使消费端处理能力不足从而导致**消息积压**







### 消息丢失的环节





<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/MessageDiscard.png" style="zoom:67%;" />



- **消息生产阶段**： 从消息被生产出来，然后提交给 `MQ` 的过程中，只要能正常收到 `MQ` Broker 的*`ack`确认响应*，就表示发送成功，所以只要处理好返回值和异常，这个阶段是不会出现消息丢失的。

- **消息存储阶段**： 这个阶段一般会直接交给 `MQ` 消息中间件来保证，但要了解它的原理，比如 *Broker 会做副本*，保证一条消息至少同步两个节点再返回 `ack`（这里涉及数据一致性原理）。

- **消息消费阶段**： 消费端从 Broker 上拉取消息，只要消费端在收到消息后，不立即发送消费确认给 Broker，而是*等到执行完业务逻辑后*，再发送消费确认，也能保证消息的不丢失。

  

> 方案看似万无一失，每个阶段都能保证消息的不丢失，但在分布式系统中，故障不可避免



作为消费生产端，你并不能保证 `MQ` 是不是弄丢了你的消息，消费者是否消费了你的消息，所以，本着 *Design for Failure* 的设计原则，你还是需要一种机制，来 Check 消息是否丢失了。





### 消息检测



#### 总体方案解决思路为

- 在消息生产端，给每个发出的消息都指定一个全局唯一 ID
- 或者附加一个连续递增的版本号
- 然后在消费端做对应的版本校验



#### 落地实现



利用拦截器机制：

- 在生产端发送消息之前，通过拦截器将消息版本号注入消息中
- 版本号可以采用连续递增的 ID 生成，也可以通过分布式全局唯一 ID生成
- 然后在消费端收到消息后，再通过拦截器检测版本号的**连续性**或**消费状态 ** 



这样实现的好处是消息检测的代码不会侵入到业务代码中，*可以通过单独的任务来定位丢失的消息*，做进一步的排查。





需要注意：

- 如果同时存在**多个**消息生产端和消息消费端，通过版本号递增的方式就很难实现了
- 因为不能保证版本号的唯一性
- 此时只能通过全局唯一 ID 的方案来进行消息检测，具体的实现原理和版本号递增的方式一致









### 消费端幂等性问题



> 在消息消费的过程中，如果出现失败的情况，通过补偿的机制发送方会执行重试，重试的过程就有可能产生重复的消息，那么如何解决这个问题？



- 幂等性，就是一条命令，任意多次执行所产生的影响均与一次执行的影响相同
- 只要消费端具备了幂等性，那么重复消费消息的问题也就解决了







最简单的实现方案，就是在数据库中建一张**消息日志表**：



<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/ReduceByMQ.png" style="zoom:67%;" />



消费消息的逻辑可以变为：

1. 在消息日志表中增加一条消息记录
2. 然后再根据消息记录，异步操作更新用户京豆余额



- 因为我们每次都会在插入之前检查是否消息已存在，所以就不会出现一条消息被执行多次的情况，这样就实现了一个幂等的操作。
- 当然，基于这个思路，不仅可以使用关系型数据库，也可以通过**`Redis`**来代替数据库实现唯一约束的方案。







### 全局唯一 ID 



> 想要解决“消息丢失”和“消息重复消费”的问题，有一个前提条件就是要实现一个全局唯一 ID 生成的技术方案



<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/GlobleUniqueID.png" style="zoom:67%;" />



- 在业务中比较倾向于选择 Snowflake 算法
- 在项目中可以对其进行改造
- 主要是让算法中的 ID 生成规则更加符合业务特点，以及优化诸如时钟回拨等问题





### 消息积压



> 如果出现积压，那一定是性能问题

- 想要解决消息从生产到消费上的性能问题：
  1. 就首先要知道**哪些环节**可能出现消息积压
  2. 然后在考虑如何解决



问题分析：

- 因为消息发送之后才会出现积压的问题，所以*和消息生产端没有关系* 
- 又因为绝大部分的消息队列单节点都能达到*每秒钟几万* 的处理能力，相对于业务逻辑来说，性能不会出现在中间件的消息存储上面
- 毫无疑问，出问题的**肯定是消息消费阶段**，那么从消费端入手，如何回答呢？





解决方案：



如果是**线上突发问题**：

- 要*临时扩容*，*增加消费端的数量*
- 与此同时，*降级一些非核心的业务* 





其次，才是**排查解决异常问题**：

- 如通过*监控，日志等手段*分析是否消费端的*业务逻辑代码*出现了问题	
- 优化消费端的业务处理逻辑





最后，如果是**消费端的处理能力不足**，可以通过*水平扩容来提供消费端的并发处理能力*。



需要特别注意：

- 在扩容消费者的实例数的同时，必须同步扩容主题 Topic 的分区数量，确保消费者的实例数和分区数相等。
- 如果消费者的实例数超过了分区数，由于分区是单线程消费，所以这样的扩容就没有效果。



比如在 Kafka 中：

- 一个 Topic 可以配置多个 Partition（分区），数据会被写入到多个分区中
- 但在消费的时候，Kafka 约定一个分区只能被一个消费者消费，Topic 的分区数量决定了消费的能力
- 所以，可以通过增加分区来提高消费者的处理能力









## 高可用



<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/HighAvailable.png" style="zoom:67%;" />







## 高性能





<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/PropertyIndex.png" style="zoom:67%;" />



<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/ThroughputCPU.png" style="zoom:67%;" />





<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/BasciFacilityOptimize.png" style="zoom:57%;" />



<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/InternetOptimize.png" style="zoom:67%;" />



<img src="https://cdn.jsdelivr.net/gh/MicroWiller/photobed@master/FrameworkOptimize.png" style="zoom:67%;" />











































